{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-26T16:50:59.897941Z","iopub.execute_input":"2022-05-26T16:50:59.898349Z","iopub.status.idle":"2022-05-26T16:50:59.928020Z","shell.execute_reply.started":"2022-05-26T16:50:59.898243Z","shell.execute_reply":"2022-05-26T16:50:59.927305Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Brief Description of Problem\n\nThis is an introductory competition on Kaggle to learn Natural Language Processing (NLP) techniques.  We are given tweets and have to determine if they are actually announcing disaster or not.  This is a simple binary classification (only 2 categories).","metadata":{}},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"#### Load Dataset","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:51:14.530012Z","iopub.execute_input":"2022-05-26T16:51:14.530328Z","iopub.status.idle":"2022-05-26T16:51:14.598450Z","shell.execute_reply.started":"2022-05-26T16:51:14.530295Z","shell.execute_reply":"2022-05-26T16:51:14.597428Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:51:20.267851Z","iopub.execute_input":"2022-05-26T16:51:20.268228Z","iopub.status.idle":"2022-05-26T16:51:20.294936Z","shell.execute_reply.started":"2022-05-26T16:51:20.268169Z","shell.execute_reply":"2022-05-26T16:51:20.294027Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:51:24.636295Z","iopub.execute_input":"2022-05-26T16:51:24.636835Z","iopub.status.idle":"2022-05-26T16:51:24.649227Z","shell.execute_reply.started":"2022-05-26T16:51:24.636787Z","shell.execute_reply":"2022-05-26T16:51:24.648261Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"len(test)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:51:28.764645Z","iopub.execute_input":"2022-05-26T16:51:28.765010Z","iopub.status.idle":"2022-05-26T16:51:28.771824Z","shell.execute_reply.started":"2022-05-26T16:51:28.764970Z","shell.execute_reply":"2022-05-26T16:51:28.770866Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Since we only want to train on the text, I will remove the columns 'id', 'keyword', and 'location'.  The target column will be our labels.","metadata":{}},{"cell_type":"code","source":"train = train.drop(['id', 'keyword', 'location'], axis=1)\ntest = test.drop(['id', 'keyword', 'location'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:51:35.953536Z","iopub.execute_input":"2022-05-26T16:51:35.953849Z","iopub.status.idle":"2022-05-26T16:51:35.967945Z","shell.execute_reply.started":"2022-05-26T16:51:35.953811Z","shell.execute_reply":"2022-05-26T16:51:35.966772Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:51:44.724348Z","iopub.execute_input":"2022-05-26T16:51:44.724749Z","iopub.status.idle":"2022-05-26T16:51:44.731040Z","shell.execute_reply.started":"2022-05-26T16:51:44.724701Z","shell.execute_reply":"2022-05-26T16:51:44.730138Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:51:49.812445Z","iopub.execute_input":"2022-05-26T16:51:49.812741Z","iopub.status.idle":"2022-05-26T16:51:49.826169Z","shell.execute_reply.started":"2022-05-26T16:51:49.812711Z","shell.execute_reply":"2022-05-26T16:51:49.825384Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing Data","metadata":{}},{"cell_type":"markdown","source":"We see the training dataset is somewhat imbalanced.  I will further clean the training data before handling this imbalance.\n\nI want to look at more entries to see what characters/cleaning may be necessary.  I will select 50 random entries from the target set. ","metadata":{}},{"cell_type":"code","source":"import random\n\nrand_idx = random.sample(list(train.index),50)\n\nfor idx in rand_idx:\n    print(train.iloc[idx,0])\n","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:52:08.836538Z","iopub.execute_input":"2022-05-26T16:52:08.836892Z","iopub.status.idle":"2022-05-26T16:52:08.856838Z","shell.execute_reply.started":"2022-05-26T16:52:08.836858Z","shell.execute_reply":"2022-05-26T16:52:08.855546Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"So, we see there are a lot of links (http://...) and tweets directed at certain users.  Removing html links and strings that begin with @ will be the initial set for cleaning.","metadata":{"execution":{"iopub.status.busy":"2022-05-25T13:42:38.382803Z","iopub.execute_input":"2022-05-25T13:42:38.383301Z","iopub.status.idle":"2022-05-25T13:42:38.399325Z","shell.execute_reply.started":"2022-05-25T13:42:38.383269Z","shell.execute_reply":"2022-05-25T13:42:38.397957Z"}}},{"cell_type":"code","source":"import re","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:52:31.566366Z","iopub.execute_input":"2022-05-26T16:52:31.566698Z","iopub.status.idle":"2022-05-26T16:52:31.571438Z","shell.execute_reply.started":"2022-05-26T16:52:31.566663Z","shell.execute_reply":"2022-05-26T16:52:31.570475Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def remove_links(sentence):\n    link = re.compile(r'https?://\\S+')\n    return link.sub(r'', sentence)\n\ndef remove_targeted_tweets(sentence):\n    tgt_twt = re.compile(r'@\\S+')\n    return tgt_twt.sub(r'', sentence)\n\ndef clean_data(data):\n    data['text'] = data['text'].apply(lambda x : remove_links(x))\n    data['text'] = data['text'].apply(lambda x : remove_targeted_tweets(x))    \n    return data","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:52:51.895683Z","iopub.execute_input":"2022-05-26T16:52:51.896001Z","iopub.status.idle":"2022-05-26T16:52:51.903960Z","shell.execute_reply.started":"2022-05-26T16:52:51.895966Z","shell.execute_reply":"2022-05-26T16:52:51.902697Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_cleaned = clean_data(train)\ntest_cleaned = clean_data(test)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:53:06.316752Z","iopub.execute_input":"2022-05-26T16:53:06.317084Z","iopub.status.idle":"2022-05-26T16:53:06.375511Z","shell.execute_reply.started":"2022-05-26T16:53:06.317052Z","shell.execute_reply":"2022-05-26T16:53:06.374692Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Now I will look at the same subset again","metadata":{}},{"cell_type":"code","source":"for idx in rand_idx:\n    print(train_cleaned.iloc[idx,0])","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:53:14.269691Z","iopub.execute_input":"2022-05-26T16:53:14.270156Z","iopub.status.idle":"2022-05-26T16:53:14.286961Z","shell.execute_reply.started":"2022-05-26T16:53:14.270101Z","shell.execute_reply":"2022-05-26T16:53:14.283848Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"So, we see the links and targets of tweets have been removed, leaving just the information from the \"body\" of the tweet.\n\nNext, I will check for duplicates and attempt to remove them as well.","metadata":{}},{"cell_type":"code","source":"train_cleaned = train_cleaned.drop_duplicates(subset='text', keep=\"first\")","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:53:24.263293Z","iopub.execute_input":"2022-05-26T16:53:24.263623Z","iopub.status.idle":"2022-05-26T16:53:24.277358Z","shell.execute_reply.started":"2022-05-26T16:53:24.263591Z","shell.execute_reply":"2022-05-26T16:53:24.276467Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(train_cleaned.shape)\nprint(test_cleaned.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:53:27.952476Z","iopub.execute_input":"2022-05-26T16:53:27.952817Z","iopub.status.idle":"2022-05-26T16:53:27.958332Z","shell.execute_reply.started":"2022-05-26T16:53:27.952783Z","shell.execute_reply":"2022-05-26T16:53:27.957141Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Ok, so we have removed duplicate entries and now we can check the class balances again.  If they are imbalanced, I will undersample the majority class.","metadata":{}},{"cell_type":"code","source":"train_cleaned['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:53:32.636707Z","iopub.execute_input":"2022-05-26T16:53:32.637015Z","iopub.status.idle":"2022-05-26T16:53:32.646786Z","shell.execute_reply.started":"2022-05-26T16:53:32.636984Z","shell.execute_reply":"2022-05-26T16:53:32.645293Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#Undersmaple majority class\nclass0 = train_cleaned[train_cleaned['target']==0]\nclass1 = train_cleaned[train_cleaned['target']==1]\n\nclass0_sample = class0.sample(n=class1.shape[0])\n","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:54:02.947537Z","iopub.execute_input":"2022-05-26T16:54:02.947845Z","iopub.status.idle":"2022-05-26T16:54:02.959093Z","shell.execute_reply.started":"2022-05-26T16:54:02.947814Z","shell.execute_reply":"2022-05-26T16:54:02.958291Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train_cleaned_balanced = pd.concat([class0_sample,class1]).sample(frac=1, random_state=12345).reset_index(drop=True)\ntrain_cleaned_balanced.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:54:08.578062Z","iopub.execute_input":"2022-05-26T16:54:08.578423Z","iopub.status.idle":"2022-05-26T16:54:08.596122Z","shell.execute_reply.started":"2022-05-26T16:54:08.578389Z","shell.execute_reply":"2022-05-26T16:54:08.595406Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"train_cleaned_balanced['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:54:12.633369Z","iopub.execute_input":"2022-05-26T16:54:12.634177Z","iopub.status.idle":"2022-05-26T16:54:12.644308Z","shell.execute_reply.started":"2022-05-26T16:54:12.634122Z","shell.execute_reply":"2022-05-26T16:54:12.643017Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Next I will look to remove \"stop words\".   These are the words in the english language that typically provide little information (such as a, an, the, etc.).","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\n\n\ndef remove_stop_words(sentence):\n    SENTENCE = sentence.split()\n    WORDS = [word for word in SENTENCE if word not in stopwords.words('english')]\n    \n    return ' '.join(WORDS)\n\ndef clean_stop_words(data):\n    data['text'] = data['text'].apply(lambda x : remove_stop_words(x))   \n    return data\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:54:17.319423Z","iopub.execute_input":"2022-05-26T16:54:17.319746Z","iopub.status.idle":"2022-05-26T16:54:18.640227Z","shell.execute_reply.started":"2022-05-26T16:54:17.319701Z","shell.execute_reply":"2022-05-26T16:54:18.639047Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"train_cleaned_balanced = clean_data(train_cleaned_balanced)\ntest_cleaned = clean_data(test_cleaned)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:54:23.639342Z","iopub.execute_input":"2022-05-26T16:54:23.639628Z","iopub.status.idle":"2022-05-26T16:54:23.684053Z","shell.execute_reply.started":"2022-05-26T16:54:23.639591Z","shell.execute_reply":"2022-05-26T16:54:23.683346Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"train_cleaned_balanced.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:54:26.573659Z","iopub.execute_input":"2022-05-26T16:54:26.574140Z","iopub.status.idle":"2022-05-26T16:54:26.585814Z","shell.execute_reply.started":"2022-05-26T16:54:26.574102Z","shell.execute_reply":"2022-05-26T16:54:26.584290Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"The final step in preprocessing the data will be to convert the text into a format the model will be able to understand.  This is called tokenization. I have included both the training and test sets in the corpus of text in order to make sure all words are included in both sets.  In a real world example, where we do not have the test set in advance, I would only be able to classify words previously seen.  Padding extends the observations with blank spaces at the end of the sentence in order for all observations to be the same length.\n\nI used the built in keras tokenizer instead of other formats.  There were pros and cons to this decision, with the primary pro being I am able to learn a bit more about tensorflow and tensors which is something I want to learn more about as I study deep learning.  The major con and something that likely is affecting the results is the lack of n-grams in this tokenizer.  \"n-grams\" are combinations of words which together may be more informative than when found apart.  For instance, the bigram \"Hurricane Warning\" may immediately indicate a pending disaster while \"That restaurants Hurricane cocktail should have come with a warning\" does not, but it uses both the words hurricane and warning.\n\nIn hindsight I probably should have used bigrams and trigrams, but for this assignment I really wanted to focus on understanding the model architecture rather than trying to perfect my score.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\n\ntext_corpus = pd.concat([train_cleaned_balanced['text'],test_cleaned['text']])\n    \ntokenizer = tf.keras.preprocessing.text.Tokenizer()\ntokenizer.fit_on_texts(text_corpus)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:54:36.346975Z","iopub.execute_input":"2022-05-26T16:54:36.347532Z","iopub.status.idle":"2022-05-26T16:54:43.934778Z","shell.execute_reply.started":"2022-05-26T16:54:36.347422Z","shell.execute_reply":"2022-05-26T16:54:43.933789Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"max_len = max(len(x.split()) for x in text_corpus)\nmax_len","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:55:46.336876Z","iopub.execute_input":"2022-05-26T16:55:46.337459Z","iopub.status.idle":"2022-05-26T16:55:46.356772Z","shell.execute_reply.started":"2022-05-26T16:55:46.337403Z","shell.execute_reply":"2022-05-26T16:55:46.355872Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"train_features = train_cleaned_balanced.iloc[:,0]\ntrain_labels = train_cleaned_balanced.iloc[:,1]\ntest_features = test_cleaned.iloc[:,0]","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:56:01.627104Z","iopub.execute_input":"2022-05-26T16:56:01.627467Z","iopub.status.idle":"2022-05-26T16:56:01.634045Z","shell.execute_reply.started":"2022-05-26T16:56:01.627420Z","shell.execute_reply":"2022-05-26T16:56:01.632583Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"train_token = tokenizer.texts_to_sequences(train_features)\ntest_token = tokenizer.texts_to_sequences(test_features)\n\ntrain_pad = pad_sequences(train_token, maxlen=max_len, padding='post')\ntest_pad = pad_sequences(test_token, maxlen=max_len, padding='post')","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:56:07.226686Z","iopub.execute_input":"2022-05-26T16:56:07.226997Z","iopub.status.idle":"2022-05-26T16:56:07.454035Z","shell.execute_reply.started":"2022-05-26T16:56:07.226965Z","shell.execute_reply":"2022-05-26T16:56:07.452681Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"train_labels = np.array(train_labels)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:56:10.564461Z","iopub.execute_input":"2022-05-26T16:56:10.564819Z","iopub.status.idle":"2022-05-26T16:56:10.569287Z","shell.execute_reply.started":"2022-05-26T16:56:10.564786Z","shell.execute_reply":"2022-05-26T16:56:10.568162Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# Model Architecture\n\nI will be be using LSTM, one of the more advanced architectures from the RNN family.  The LSTMs let the model remember inputs over longer periods of time.  I felt this was useful since most on twitter do not write in full sentences but rather snippets.  By remembering how to treat these smaller snippets and improper grammar the model may be able to perform a bit better.  I will follow these up by some dense/fully connected layers as I find those generally aid with classification.","metadata":{}},{"cell_type":"code","source":"import keras\nfrom keras.layers import LSTM\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, Bidirectional, Dropout\nfrom keras import optimizers","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:56:20.485856Z","iopub.execute_input":"2022-05-26T16:56:20.486237Z","iopub.status.idle":"2022-05-26T16:56:20.491244Z","shell.execute_reply.started":"2022-05-26T16:56:20.486175Z","shell.execute_reply":"2022-05-26T16:56:20.490311Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"all_words = len(tokenizer.word_index)+1\nembedding_units = 100\nhidden_units = 64\n\nmodel0 = Sequential()\nmodel0.add(Embedding(all_words, embedding_units, input_length = max_len))\nmodel0.add(Bidirectional(LSTM(hidden_units)))\nmodel0.add(Dense(128, activation='tanh'))\nmodel0.add(Dense(64, activation='tanh'))\nmodel0.add(Dense(1, activation='sigmoid'))\n\nmodel0.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:56:25.899641Z","iopub.execute_input":"2022-05-26T16:56:25.900342Z","iopub.status.idle":"2022-05-26T16:56:26.620981Z","shell.execute_reply.started":"2022-05-26T16:56:25.900295Z","shell.execute_reply":"2022-05-26T16:56:26.620113Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"model0.compile( loss=tf.keras.losses.BinaryCrossentropy(),\n    optimizer=tf.keras.optimizers.Adam(0.00001),\n    metrics=['accuracy', 'Precision', 'Recall'])","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:56:40.369730Z","iopub.execute_input":"2022-05-26T16:56:40.370117Z","iopub.status.idle":"2022-05-26T16:56:40.392007Z","shell.execute_reply.started":"2022-05-26T16:56:40.370074Z","shell.execute_reply":"2022-05-26T16:56:40.391145Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"#added a callback for early stopping if it appears to be overfitting based on val_loss\ncall_backs = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1)]","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:56:45.132156Z","iopub.execute_input":"2022-05-26T16:56:45.133189Z","iopub.status.idle":"2022-05-26T16:56:45.138611Z","shell.execute_reply.started":"2022-05-26T16:56:45.133141Z","shell.execute_reply":"2022-05-26T16:56:45.137276Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"history0 = model0.fit(train_pad, train_labels,epochs=50,validation_split=0.2, callbacks = call_backs)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:56:55.291362Z","iopub.execute_input":"2022-05-26T16:56:55.291709Z","iopub.status.idle":"2022-05-26T17:00:16.518979Z","shell.execute_reply.started":"2022-05-26T16:56:55.291675Z","shell.execute_reply":"2022-05-26T17:00:16.517555Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfig, axs = plt.subplots(1, 4, figsize=(20, 5))\n\naxs[0].set_title('Loss')\naxs[0].plot(history0.history['loss'], label='train')\naxs[0].plot(history0.history['val_loss'], label='val')\naxs[0].legend()\n\naxs[1].set_title('Accuracy')\naxs[1].plot(history0.history['accuracy'], label='train')\naxs[1].plot(history0.history['val_accuracy'], label='val')\naxs[1].legend()\n\naxs[2].set_title('Precision')\naxs[2].plot(history0.history['precision'], label='train')\naxs[2].plot(history0.history['val_precision'], label='val')\naxs[2].legend()\n\naxs[3].set_title('Recall')\naxs[3].plot(history0.history['recall'], label='train')\naxs[3].plot(history0.history['val_recall'], label='val')\naxs[3].legend()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T17:03:09.943658Z","iopub.execute_input":"2022-05-26T17:03:09.943963Z","iopub.status.idle":"2022-05-26T17:03:10.598675Z","shell.execute_reply.started":"2022-05-26T17:03:09.943932Z","shell.execute_reply":"2022-05-26T17:03:10.597281Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"This model did not utilize any dropout for generalization and we can see that there appears to be some overfitting around the 15th epoch and the model stopped before 25 epochs were even completed based on the early stopping callbacks set up.  I will next attempt to add some dropout for generalization.","metadata":{}},{"cell_type":"code","source":"model1 = Sequential()\nmodel1.add(Embedding(all_words, embedding_units, input_length = max_len))\nmodel1.add(Bidirectional(LSTM(hidden_units)))\nmodel1.add(Dropout(0.2))\nmodel1.add(Dense(128, activation='tanh'))\nmodel1.add(Dropout(0.2))\nmodel1.add(Dense(64, activation='tanh'))\nmodel1.add(Dropout(0.2))\nmodel1.add(Dense(1, activation='sigmoid'))\n\nmodel1.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T17:06:46.717933Z","iopub.execute_input":"2022-05-26T17:06:46.718613Z","iopub.status.idle":"2022-05-26T17:06:47.265403Z","shell.execute_reply.started":"2022-05-26T17:06:46.718561Z","shell.execute_reply":"2022-05-26T17:06:47.264015Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"model1.compile( loss=tf.keras.losses.BinaryCrossentropy(),\n    optimizer=tf.keras.optimizers.Adam(0.00001),\n    metrics=['accuracy', 'Precision', 'Recall'])","metadata":{"execution":{"iopub.status.busy":"2022-05-26T17:06:53.310944Z","iopub.execute_input":"2022-05-26T17:06:53.311276Z","iopub.status.idle":"2022-05-26T17:06:53.323864Z","shell.execute_reply.started":"2022-05-26T17:06:53.311240Z","shell.execute_reply":"2022-05-26T17:06:53.322577Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"history1 = model1.fit(train_pad, train_labels,epochs=50,validation_split=0.2, callbacks = call_backs)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T17:07:03.958910Z","iopub.execute_input":"2022-05-26T17:07:03.960162Z","iopub.status.idle":"2022-05-26T17:10:51.613485Z","shell.execute_reply.started":"2022-05-26T17:07:03.960113Z","shell.execute_reply":"2022-05-26T17:10:51.611781Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfig, axs = plt.subplots(1, 4, figsize=(20, 5))\n\naxs[0].set_title('Loss')\naxs[0].plot(history1.history['loss'], label='train')\naxs[0].plot(history1.history['val_loss'], label='val')\naxs[0].legend()\n\naxs[1].set_title('Accuracy')\naxs[1].plot(history1.history['accuracy'], label='train')\naxs[1].plot(history1.history['val_accuracy'], label='val')\naxs[1].legend()\n\naxs[2].set_title('Precision')\naxs[2].plot(history1.history['precision'], label='train')\naxs[2].plot(history1.history['val_precision'], label='val')\naxs[2].legend()\n\naxs[3].set_title('Recall')\naxs[3].plot(history1.history['recall'], label='train')\naxs[3].plot(history1.history['val_recall'], label='val')\naxs[3].legend()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T17:11:25.672463Z","iopub.execute_input":"2022-05-26T17:11:25.673103Z","iopub.status.idle":"2022-05-26T17:11:26.264259Z","shell.execute_reply.started":"2022-05-26T17:11:25.673063Z","shell.execute_reply":"2022-05-26T17:11:26.263284Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"We see adding some generalization techniques helped the model learn a bit better as the loss wasnt so clearly overfitting to the training set and the accuracy on the validation set did improve some as well.  In fact, all of the metrics graphed above are better than the first model.  Next, I will change the activation from 'tanh' to 'relu' and see if adjusting that hyperparameter further improves the model.","metadata":{}},{"cell_type":"code","source":"model2 = Sequential()\nmodel2.add(Embedding(all_words, embedding_units, input_length = max_len))\nmodel2.add(Bidirectional(LSTM(hidden_units)))\nmodel2.add(Dropout(0.2))\nmodel2.add(Dense(128, activation='relu'))\nmodel2.add(Dropout(0.2))\nmodel2.add(Dense(64, activation='relu'))\nmodel2.add(Dropout(0.2))\nmodel2.add(Dense(1, activation='sigmoid'))\n\nmodel2.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T17:17:16.089538Z","iopub.execute_input":"2022-05-26T17:17:16.089978Z","iopub.status.idle":"2022-05-26T17:17:16.703009Z","shell.execute_reply.started":"2022-05-26T17:17:16.089944Z","shell.execute_reply":"2022-05-26T17:17:16.701424Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"model2.compile( loss=tf.keras.losses.BinaryCrossentropy(),\n    optimizer=tf.keras.optimizers.Adam(0.00001),\n    metrics=['accuracy', 'Precision', 'Recall'])","metadata":{"execution":{"iopub.status.busy":"2022-05-26T17:17:26.501863Z","iopub.execute_input":"2022-05-26T17:17:26.502217Z","iopub.status.idle":"2022-05-26T17:17:26.514750Z","shell.execute_reply.started":"2022-05-26T17:17:26.502167Z","shell.execute_reply":"2022-05-26T17:17:26.513926Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"history2 = model2.fit(train_pad, train_labels,epochs=50,validation_split=0.2, callbacks = call_backs)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T17:17:29.601578Z","iopub.execute_input":"2022-05-26T17:17:29.602472Z","iopub.status.idle":"2022-05-26T17:21:31.943155Z","shell.execute_reply.started":"2022-05-26T17:17:29.602427Z","shell.execute_reply":"2022-05-26T17:21:31.941503Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfig, axs = plt.subplots(1, 4, figsize=(20, 5))\n\naxs[0].set_title('Loss')\naxs[0].plot(history2.history['loss'], label='train')\naxs[0].plot(history2.history['val_loss'], label='val')\naxs[0].legend()\n\naxs[1].set_title('Accuracy')\naxs[1].plot(history2.history['accuracy'], label='train')\naxs[1].plot(history2.history['val_accuracy'], label='val')\naxs[1].legend()\n\naxs[2].set_title('Precision')\naxs[2].plot(history2.history['precision'], label='train')\naxs[2].plot(history2.history['val_precision'], label='val')\naxs[2].legend()\n\naxs[3].set_title('Recall')\naxs[3].plot(history2.history['recall'], label='train')\naxs[3].plot(history2.history['val_recall'], label='val')\naxs[3].legend()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T17:21:31.945139Z","iopub.execute_input":"2022-05-26T17:21:31.945442Z","iopub.status.idle":"2022-05-26T17:21:32.550847Z","shell.execute_reply.started":"2022-05-26T17:21:31.945409Z","shell.execute_reply":"2022-05-26T17:21:32.549762Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"So, we see the Relu activations didnt really change the performance at all. Finally, looking to see how performance improves with more layers of LSTMs, I will add a couple more and compare the results there.","metadata":{}},{"cell_type":"code","source":"model3 = Sequential()\nmodel3.add(Embedding(all_words, embedding_units, input_length = max_len))\nmodel3.add(Bidirectional(LSTM(hidden_units,return_sequences=True)))\nmodel3.add(Bidirectional(LSTM(hidden_units,return_sequences=True)))\nmodel3.add(Bidirectional(LSTM(hidden_units)))\nmodel3.add(Dropout(0.2))\nmodel3.add(Dense(128, activation='relu'))\nmodel3.add(Dropout(0.2))\nmodel3.add(Dense(64, activation='relu'))\nmodel3.add(Dropout(0.2))\nmodel3.add(Dense(1, activation='sigmoid'))\n\nmodel3.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:04:17.229725Z","iopub.execute_input":"2022-05-26T18:04:17.230047Z","iopub.status.idle":"2022-05-26T18:04:18.745357Z","shell.execute_reply.started":"2022-05-26T18:04:17.230014Z","shell.execute_reply":"2022-05-26T18:04:18.744388Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"model3.compile( loss=tf.keras.losses.BinaryCrossentropy(),\n    optimizer=tf.keras.optimizers.Adam(0.00001),\n    metrics=['accuracy', 'Precision', 'Recall'])","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:04:22.766010Z","iopub.execute_input":"2022-05-26T18:04:22.766897Z","iopub.status.idle":"2022-05-26T18:04:22.780858Z","shell.execute_reply.started":"2022-05-26T18:04:22.766823Z","shell.execute_reply":"2022-05-26T18:04:22.780001Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"history3 = model3.fit(train_pad, train_labels,epochs=50,validation_split=0.2, callbacks = call_backs)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:04:26.383044Z","iopub.execute_input":"2022-05-26T18:04:26.383942Z","iopub.status.idle":"2022-05-26T18:10:42.605497Z","shell.execute_reply.started":"2022-05-26T18:04:26.383862Z","shell.execute_reply":"2022-05-26T18:10:42.604758Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n\naxs[0].set_title('Loss')\naxs[0].plot(history3.history['loss'], label='train')\naxs[0].plot(history3.history['val_loss'], label='val')\naxs[0].legend()\n\naxs[1].set_title('Accuracy')\naxs[1].plot(history3.history['accuracy'], label='train')\naxs[1].plot(history3.history['val_accuracy'], label='val')\naxs[1].legend()\n\naxs[2].set_title('Precision')\naxs[2].plot(history3.history['precision'], label='train')\naxs[2].plot(history3.history['val_precision'], label='val')\naxs[2].legend()\n\naxs[3].set_title('Recall')\naxs[3].plot(history3.history['recall'], label='train')\naxs[3].plot(history3.history['val_recall'], label='val')\naxs[3].legend()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:02:11.428026Z","iopub.execute_input":"2022-05-26T18:02:11.429624Z","iopub.status.idle":"2022-05-26T18:02:12.065612Z","shell.execute_reply.started":"2022-05-26T18:02:11.429559Z","shell.execute_reply":"2022-05-26T18:02:12.064685Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"We see adding extra layers of LSTMs does not actually improve the results and appears to make them less consistent on the validation set.","metadata":{}},{"cell_type":"markdown","source":"# Results and Analysis\n\nWe observed similar results across different architectures and hyperparameters.  I will use model2 above as the final model for submission as it was arguably the best of the models I compared.","metadata":{}},{"cell_type":"code","source":"preds = model2.predict(test_pad)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:16:28.061565Z","iopub.execute_input":"2022-05-26T18:16:28.061911Z","iopub.status.idle":"2022-05-26T18:16:30.219391Z","shell.execute_reply.started":"2022-05-26T18:16:28.061876Z","shell.execute_reply":"2022-05-26T18:16:30.218634Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"len(preds)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:16:31.993569Z","iopub.execute_input":"2022-05-26T18:16:31.994682Z","iopub.status.idle":"2022-05-26T18:16:32.002165Z","shell.execute_reply.started":"2022-05-26T18:16:31.994632Z","shell.execute_reply":"2022-05-26T18:16:31.999661Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"predictions = []\n\nfor pred in preds:\n    if pred >= 0.5:\n        predictions.append(1)\n    else:\n        predictions.append(0)\n        \npredictions[:10]","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:16:34.351604Z","iopub.execute_input":"2022-05-26T18:16:34.351947Z","iopub.status.idle":"2022-05-26T18:16:34.368183Z","shell.execute_reply.started":"2022-05-26T18:16:34.351910Z","shell.execute_reply":"2022-05-26T18:16:34.367268Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\nsubmission","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:16:43.860361Z","iopub.execute_input":"2022-05-26T18:16:43.860672Z","iopub.status.idle":"2022-05-26T18:16:43.889638Z","shell.execute_reply.started":"2022-05-26T18:16:43.860638Z","shell.execute_reply":"2022-05-26T18:16:43.888689Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"submission['target']=predictions\nsubmission","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:16:47.900091Z","iopub.execute_input":"2022-05-26T18:16:47.900609Z","iopub.status.idle":"2022-05-26T18:16:47.915359Z","shell.execute_reply.started":"2022-05-26T18:16:47.900556Z","shell.execute_reply":"2022-05-26T18:16:47.914040Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:17:03.742925Z","iopub.execute_input":"2022-05-26T18:17:03.743664Z","iopub.status.idle":"2022-05-26T18:17:03.762775Z","shell.execute_reply.started":"2022-05-26T18:17:03.743602Z","shell.execute_reply":"2022-05-26T18:17:03.761115Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nSo, I see that over all of the different architectures and parameters my accuracy hovered around 75%.  I do think using different methods for cleaning and pre-processing the data may help improve that as well as further analysis study and a better understanding of how best to improve the architecture of the neural network.","metadata":{}},{"cell_type":"markdown","source":"#### References\n\nAlong with the documentation for tensorflow, keras, and numpy, I also used these resources found on kaggle:\n\nhttps://www.kaggle.com/code/msondkar/disaster-tweets-classification-with-an-rnn/notebook\n\nhttps://www.kaggle.com/code/mattbast/rnn-and-nlp-detect-a-disaster-in-tweets/notebook#Encode-sentences","metadata":{}}]}